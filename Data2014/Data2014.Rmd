---
title: "Region1CyanoData2014"
author: "Bryan Milstead"
date: "March 18, 2015"
output: html_document
---
<!---
use these command instead of the knit icon if you want the data and work loaded into the R workspace
  library(knitr)
    setwd("Data2014")
    knit('Data2014.rmd')
-->
To Do List
-------------------------
* Convert RFU to ug/l for CTDEEP and UNH_CFB
* Map locations and send to contacts for verifications


Introduction
-------------------------
During the summer of 2014 the New England states initiated a monitoring program for cyanobacteria in lakes.  Participants included state and tribal governments, local watershed associations, EPA Region 1, and the EPA Atlantic Ecology Division. 

The task now is to collate all the data into a standardized format.  Below are the data processing steps used and some questions to be resolved for each data contributor.

A simple excel template for data entry is available here: https://github.com/willbmisled/Reg1Cyano/blob/master/Data2014/Region1CyanobacteriaDataEntryTemplate.xls?raw=true

This document is available here: https://github.com/willbmisled/Reg1Cyano/blob/master/Data2014/Data2014.md

The details of all data processing steps including notes and rcode are available here: https://github.com/willbmisled/Reg1Cyano/blob/master/Data2014/Data2014.Rmd

The collated dataset is available here in .csv format (hint: right click on the link and choose “Save As” to save the file to your computer, otherwise, it will open in your browser): https://github.com/willbmisled/Reg1Cyano/blob/master/Data2014/Data2014.csv?raw=true

A document showing how the locations and WBID's were defined and where to download a KML file of the locations can be found here: https://github.com/willbmisled/Reg1Cyano/blob/master/Data2014/Locations2014.md

Data Definitions
-------------------------

Field  | Units | Description
------------- | ------------- | -------------
**Order:**|(integer)|Unique Identifier for sample by Organization-disregard this is for house keeping only.
**YourName:**|(text)|Enter the name of the person entering the data
**Organization:**|(text)|Enter the name of the organization responsible for collecting the samples-e.g. Vermont DEC
**SiteID:**|(text)|If the location has a site ID from your organization enter it here
**WaterbodyName:**|(text)|Enter the name of the lake: be consistent with spelling and capitalization
**State:**|(lookup)|Choose the two letter state code from the drop down list
**SiteLocation:**|(text)|Some organizations have site names within a lake.  Enter these here.
**SampleYear:**|(YYYY)|Year sample was collected in format YYYY (e.g., 2014)
**SampleMonth:**|(MM)|Month sample was collected in integer format (e.g., months 1 to 12 )
**SampleDay:**|(DD)|Day sample was collected in integer format (e.g., day 1 to 31 )
**NameOfSamplers:**|(text)|Add the names of the field crew separated by commas.
**WeatherConditions:**|(text)|General Weather conditions separated by commas.  E.g., Cloudy, Windy, Cold
**SampleLocation:**|(lookup)|Where (in the lake) was the sample collected followed by the replicate number? WithinLake=WL1, WL2, or WL3; ShoreSide=SS1, SS2, SS3; Can also add Calibration or Blank for validation readings or Other.
**SampleMethod:**|(lookup)|How was the sample collected? Grab = Grab sample for surface blooms; Composite =Composite; Integrated = Integrated tube sample; Validation = Use this for Blanks and Calibration Standards; Other = give details in the comments section.
**Depth:**|(integer)|Within Lake samples are at 3m and Shoreside are at 1m.  If samples taken at different depths not the depth here and enter details in the comments section.  Leave blank for standards and blanks.
**Latitude:**|(decimal)|Latitude in Decimal Degrees with 4 decimal places or degrees minutes seconds
**Longitude:**|(decimal)|Longitude in Decimal Degrees with 4 decimal places or degrees minutes seconds
**SampleHour:**|(HH)|Hour sample was taken in 24 hour format
**SampleMinutes:**|(MM)|Minute sample was taken in integer format
**Filtered:**|(TRUE/FALSE)|Was the sample filtered?
**Frozen:**|(TRUE/FALSE)|Was the sample frozen?
**Parameter:**|(lookup)|Phycocyanin or Chlorophyll?
**Value:**|(decimal)|Reading from the fluorometer
**Units:**|(lookup)|What were the units recorded.  If not "RFU" or "µ/L" flag the entry and add comment with the units used. 
**Rep:**|(integer)|If you made more than one measurement per sample or took more than one sample per site assign a replicate number to each observations and add notes in the comment field.
**Fluorometer:**|(lookup)|Type of fluorometer used?
**AnalysisYear:**|(YYYY)|Year sample was analyzed in format YYYY (e.g., 2014)
**AnalysisMonth:**|(MM)|Month sample was analyzed in integer format (e.g., months 1 to 12 )
**AnalysisDay:**|(DD)|Day sample was analyzed in integer format (e.g., day 1 to 31 )
**AnalysisHour:**|(HH)|Hour sample was analyzed in 24 hour format
**AnalysisMinutes:**|(MM)|Minute sample was analyzed in integer format
**GPSType:**|(text)|How was the location determined.  GPS (type), map, or google?
**Photos:**|(TRUE/FALSE)|Where photos taken?
**Flag:**|(TRUE/FALSE)|Add a flag for any data line that needs further validation or processing
**Comments:**|(text)|Add details of flags or any notes about the data line or sample.
**ID:**|(integer)|Unique Identifier for Sample
**LocID:**|(integer)|Unique Identifier for the Location based on unique Lon/Lat combinations

Data Sources
-------------------------
Information on how the data were provided by the States and Organizations that participated in the monitoring.


### Vermont

* Data received from Hilary Snook (forwarded from Angela Shambaugh-VTDEC) on 11/13/14
* Original file name: VERMONT Beagle FluoroQuick data_Summer 2014.xlsx renamed VTDEC.xlsx
* Responses to my queries answered by Angela Shambaugh on 12/19/2014

**Questions for Angela with responses from Angela (AS), Hilary Snook (HS) and me (BM):**

* Do we need to treat the two groups of data ("calibrated pigment readings" & "after calibration loss") separately?
    - AS: Depends on Hilary’s experience when re-calibrating my Beagle unit.  I don’t know if he was able to tell how much drift there was over the summer.
    - HS: I’ll hopefully be checking these out today. 
    - BM: tables combined and the "after calibration loss" lines flagged.
* What are the differences between the two groups of data?
    - AS: I apparently invalidated the instrument calibration halfway through the summer by mistakenly starting the full calibration process and then incorrectly exiting.  The check standards Hilary provided were not at all consistent with expected reading so I stopped reporting phycocyanin units and switched to relative fluorescence units.  
* What is the difference between "blank" and "blank(2)" in the "station" field?
    - AS: Blank refers to a DI sample read at the beginning of each session, blank 2 refers to a second reading of the same sample at the end of the session.
    - BM: leave as is for now.
* For parameter what do: "channel 1 hi", "channel 1 lo", and channel 2 lo" represent?
    - AS: If I understood the Beagle unit correctly, Channel 1 refers to the phycocyanin filter and channel 2 to the chlorophyll filter.  It wasn’t clear to me the difference between high and low – Hilary may be able to answer that better.  Hopefully after recalibrating the instrument, we can interpolate and change all the relative fluorescent units to phycocyanin.  Currently, any number associated with a channel will be RFU.  Anything associated with the terms ‘phycocyanin’ or ‘chlorophyll’ will be biomass (as µg/L).
    - HS: Hi/Lo is automatically selected by the instrument based on sample fluorescence and does not need to be manually selected by the user. This was pretty cryptic in the user manual and I will clarify this at the upcoming meeting.
    - BM: add units="RFU"  for all samples where parameter= channel 1 or 2; add units="µg/L" for all samples where parameter="phycocyanin or chlorophyll"; change paramater="channel 1 (hi or lo)" to "Phycocyanin"; change paramater="channel 2 (hi or lo)" to "Chlorophyll"
* Where were the samples taken?  Shoreside? or Within Lake?  I'm assuming that the 1m depth samples are Shoreside and the 3m or greater depth samples are within lake but would like to verify.
    - AS: All locations from the Champlain TMDL sites (#4 – 51) will be offshore.  RT 78 Access is shoreline as is the Black Bridge location. The others ( Highgate Cliffs, Highgate Springs, Phillipsburg and QE BM-b) are all off shore.  As part of our cyanobacteria monitoring, we take only surface samples for microcystin analysis – hence any surface scums at offshore locations will be surface grabs. 
    - BM: change "RT 78 Access" and "Black Bridge" to SS1; the rest are WL1
* What are the units for fluorometer readings?
    - AS: Beagle units are µg/L, I think.  Hilary can confirm.
    - HS: Yes, readout is in micrograms/liter (ppb)
    - BM: µg/L
* For the "sample state" VTDEC reports the conditions as  c("filtered, CDOM, fresh", "fresh","frozen","DI", "24 hours old", "white", "orange", "blue","", "red", "green", "yellow","pink").  What information is important?  For each condition we need to know whether the sample was frozen (TRUE/FALSE) or filtered (TRUE/FALSE) before the fluoremeter reading was taken. 
    - AS: The colors under ‘sample state’ refer to the solid check standards provided by Hilary so the terms frozen/filtered do not apply.  The 24hr sample old sample was not frozen or filtered.  All CDOM samples were filtered and not frozen.  All fresh samples were unfrozen and unfiltered.  If it is easier, I can re-send my data using the EXCEL spreadsheet you sent out.
    - BM: all samples except those for sample.state=='Frozen' changed to Frozen=FALSE; samples for sample.state="...CDOM..." changed to Filtered==TRUE, the rest are Filtered=FALSE.
* Please check the file VTDEC_Locations.csv.  I have some of the locations but not all.  The locations I have were gleaned from the lake Champlain reports.  In some cases I used Google Earth to get the locations from the descriptions so I don't have a lot of confidence in the locations.  Can you verify the locations (I can send a map of the locations if you wish) and help me with the ones that are missing?
    - AS: I added lat lons to your table for the missing locations.  If you took the lat lon for the other stations from the Champlain TMDL files, they should be correct.  The updates are attached.

**Data steps**-View code in Data2014.rmd document to see details.

* table="calibrated pigment readings" in VTDEC.xlsx saved as VTDEC_calibrated.csv; read into R
* table="after calibration loss" in VTDEC.xlsx saved as VTDEC_uncalibrated.csv; read into R;Flag=TRUE;Comment="Uncertain Calibration Status"
* both tables combined into df=VT
* Build VTDEC data.frame
* Add to data.frame Data2014

```{r VTDEC, include=FALSE, echo=FALSE, cache=FALSE} 
#read data and combine tables
  VT<-read.csv("VTDEC_calibrated.csv",stringsAsFactors=FALSE) #read the calbibrated data
    VT$Flag<-FALSE #add Flag field
    VT$Comments<-VT$notes  #rename notes to comments
    nrow(VT) #444
  
  VT1<-read.csv("VTDEC_uncalibrated.csv") #read the uncalbrated data
    VT1$Flag<-TRUE #add Flag field
    VT1$Comments<-paste("Uncertain Calibration Status;", VT1$notes)  #add comment and notes to comments
    nrow(VT1) #392
  
  VT<-rbind(VT,VT1) #combine the datasets
    nrow(VT) #836

#Build VTDEC
  #Order=sequential ordering of the raw data
    VTDEC<-data.frame(Order=1:nrow(VT))
  #Add name and org
    VTDEC$YourName<-"Angela Shambaugh"  
    VTDEC$Organization<-"VTDEC"
  #Add SiteID from VT$station
    VTDEC$SiteID<-VT$station
    #fix spellings
      as.data.frame(table(VTDEC$SiteID,useNA='ifany'))
      VTDEC$SiteID[VTDEC$SiteID=="QE BM - b"]<-"QE BM b"
      VTDEC$SiteID[VTDEC$SiteID=="highgate Springs, VT"]<-"Highgate Springs"
    #remove blanks and standards 
      VTDEC$SiteID[grep('blank',VTDEC$SiteID)]<-NA
      VTDEC$SiteID[grep('standard',VTDEC$SiteID)]<-NA
  #Add WaterbodyName
    VTDEC$WaterbodyName<-NA
  #Add State	
    VTDEC$State<-"VT"
  #Add SiteLocation	
    VTDEC$SiteLocation<-NA
  # Add SampleYear	SampleMonth	SampleDay	
    a<-matrix(as.numeric(unlist(strsplit(VT$Sample.Date,'/'))),nrow=nrow(VT),ncol=3,byrow=TRUE)
      VTDEC$SampleYear<-a[,3] 
      VTDEC$SampleMonth<-a[,1] 
      VTDEC$SampleDay<-a[,2] 
  #Add NameOfSamplers	
    VTDEC$NameOfSamplers<-NA
  #Add WeatherConditions	
    VTDEC$WeatherConditions<-NA
  #Add SampleLocation	
    VTDEC$SampleLocation<-"WL1"
    VTDEC$SampleLocation[grep('Black',VT$station)]<-'SS1'
    VTDEC$SampleLocation[grep('Access',VT$station)]<-'SS1'
    VTDEC$SampleLocation[grep('blank',VT$station)]<-'Blank'
    VTDEC$SampleLocation[grep('standard',VT$station)]<-'Calibration'
    table(VTDEC$SampleLocation,useNA='ifany')
  #Add SampleMethod from VT$sample.type & VT&collection.type; blanks and standards from VT$station
    table(VT$sample.type,useNA='ifany') #654 whole water samples and 182 blanks and standards
    table(VT$collection.type,useNA='ifany') #grab=104;hose=550; blanks & standards=182
    VTDEC$SampleMethod<-VT$collection.type
      #rename methods to match template
        VTDEC$SampleMethod[VTDEC$SampleMethod=="grab"]<-'Grab'
        VTDEC$SampleMethod[VTDEC$SampleMethod=="hose"]<-'Integrated'
      #add blanks and standards
        VTDEC$SampleMethod[grep('blank',VT$station)]<-VT$station[grep('blank',VT$station)]
        VTDEC$SampleMethod[grep('standard',VT$station)]<-VT$station[grep('standard',VT$station)]
        table(VTDEC$SampleMethod,useNA='ifany')
   #Add Depth  
    VTDEC$Depth<-VT$depth..m.
  #Add Latitude	Longitude
    loc<-read.csv("VTDEC_Locations.csv") #read the locations
    a<-merge(VTDEC,loc,by='SiteID',all.x=T) #merge locations and siteIDs
    a<-a[order(a$Order),] #reorder
    VTDEC$Latitude<-a$Latitude
    VTDEC$Longitude<-a$Longitude
  #Add SampleHour	SampleMinutes	
    VTDEC$SampleHour<-NA
    VTDEC$SampleMinutes<-NA
  #Add Filtered	
    VTDEC$Filtered<-FALSE
    VTDEC$Filtered[grep('filtered',VT$sample.state)]<-TRUE
    table(VTDEC$Filtered)
  #Add Frozen
    VTDEC$Frozen<-FALSE
    VTDEC$Frozen[grep('frozen',VT$sample.state)]<-TRUE
    table(VTDEC$Frozen)
  #Add Parameter & Value
    VTDEC$Parameter<-VT$parameter
    VTDEC$Value<-VT$Reading
  #Add Units and update parameter
    VTDEC$Units<-'ug/l'
    #change channel 1 to Parameter=Phycocyanin and Units=RFU
      a<-grep('1',VTDEC$Parameter)
        VTDEC$Parameter[a]<-'Phycocyanin'
        VTDEC$Units[a]<-'RFU'
    #change channel 2 to Parameter=Chlorophyll and Units=RFU
      a<-grep('2',VTDEC$Parameter)
        VTDEC$Parameter[a]<-'Chlorophyll'
        VTDEC$Units[a]<-'RFU'
    #change spelling
      VTDEC$Parameter[VTDEC$Parameter=='phycocyanin']<-'Phycocyanin'
      VTDEC$Parameter[VTDEC$Parameter=='chlorophyll']<-'Chlorophyll'
  #Add Rep
    VTDEC$Rep<-VT$rep
  #Add Fluorometer
    VTDEC$Fluorometer<-'Beagle'
  #Add AnalysisYear  AnalysisMonth	AnalysisDay	AnalysisHour	AnalysisMinutes
    VT[VT$Analysis.date=='','Analysis.date']<-"NA/NA/NA" #replace missing values
    a<-matrix(as.numeric(unlist(strsplit(VT$Analysis.date,'/'))),nrow=nrow(VT),ncol=3,byrow=TRUE)
      VTDEC$AnalysisYear<-a[,3] 
      VTDEC$AnalysisMonth<-a[,1] 
      VTDEC$AnalysisDay<-a[,2] 
    VTDEC$AnalysisHour<-NA
    VTDEC$AnalysisMinutes<-NA
  #Add GPSType	Photos	Flag	Comments
    VTDEC$GPSType<-NA
    VTDEC$Photos<-FALSE
    VTDEC$Flag<-VT$Flag
    VTDEC$Comments<-VT$Comments
#add to Data2014
  Data2014<-VTDEC #this creates Data2014
```

### Charles River Watershed Association

* Data received from Elisabeth Cianciola on 12/12/14
* Original file name: CRWA_Region1CyanobacteriaDataEntryTemplate.xls renamed CRWA.xls
* Resaved as CRWA_RAW.csv for import into R

**Questions for Elisabeth with her responses**

* Were all samples collected with the integrated tube sampler?
    - EC: Yes
* What were the units for the fluorometer readings?  RFU? µg/L?
    - EC: µg/L
    
**Data steps**-View code in Data2014.rmd document to see details.

* Data originally entered in a version of Region1CyanobacteriaDataEntryTemplate.xls so little massaging was needed.
* CWRA_RAW.csv imported into R
* Each field verified with consistency checks
* Field updated where necessary.
* Add to data.frame Data2014

```{r CRWA, include=FALSE, echo=FALSE, cache=FALSE} 
#read data 
  CRWA<-read.csv("CRWA_RAW.csv",stringsAsFactors=FALSE) #read the data
  names(CRWA)[c(15,16,23,24)]<-c("Filtered1","Frozen1","Photos1","Flag1") #rename fields that ended in "?"
#check fields for consistency
  table(CRWA$YourName,useNA='ifany')
  table(CRWA$Organization,useNA='ifany')
  table(CRWA$SiteID,useNA='ifany')
  table(CRWA$WaterbodyName,useNA='ifany')
  table(CRWA$State,useNA='ifany')
  table(CRWA$SiteLocation,useNA='ifany')
  table(CRWA$NameOfSamplers,useNA='ifany')
  table(CRWA$WeatherConditions,useNA='ifany')
  table(CRWA$Depth,useNA='ifany')
  table(CRWA$Latitude,useNA='ifany')
  table(CRWA$Longitude,useNA='ifany')
  table(CRWA$Parameter,useNA='ifany')
  summary(CRWA$Value)
  table(is.na(CRWA$Value))
  table(CRWA$Fluorometer,useNA='ifany')
  table(CRWA$GPSType,useNA='ifany')
  table(CRWA$Comments,useNA='ifany')
#update or modify fields
  #Add Order=sequential ordering of the raw data
    CRWA$Order<-1:nrow(CRWA)
  # Add SampleYear	SampleMonth	SampleDay	Drop SampleDate
    a<-matrix(as.numeric(unlist(strsplit(CRWA$SampleDate,'/'))),nrow=nrow(CRWA),ncol=3,byrow=TRUE)
      CRWA$SampleYear<-a[,3] 
      CRWA$SampleMonth<-a[,1] 
      CRWA$SampleDay<-a[,2]
      CRWA<-CRWA[,-which(names(CRWA)=="SampleDate")]
  # Add AnalysisYear  AnalysisMonth	AnalysisDay	Drop AnalysisDate
    a<-matrix(as.numeric(unlist(strsplit(CRWA$AnalysisDate,'/'))),nrow=nrow(CRWA),ncol=3,byrow=TRUE)
      CRWA$AnalysisYear<-a[,3] 
      CRWA$AnalysisMonth<-a[,1] 
      CRWA$AnalysisDay<-a[,2]
      CRWA<-CRWA[,-which(names(CRWA)=="AnalysisDate")]
  #Add SampleLocation	& SampleMethod
    CRWA$SampleLocation<-"SS1" #all sites at SS1
    CRWA<-CRWA[,-which(names(CRWA)=="TypeOfSample")] #delete earlier name for this field
    CRWA$SampleMethod<-'Integrated' #all collected with tube
  #Add SampleHour	SampleMinutes	
    CRWA$SampleTime[CRWA$SampleTime==""]<-"NA:NA" #replace missing values
    a<-matrix(as.numeric(unlist(strsplit(CRWA$SampleTime,':'))),nrow=nrow(CRWA),ncol=2,byrow=TRUE) #parse
    CRWA$SampleHour<-a[,1]
    CRWA$SampleMinutes<-a[,2]
    CRWA<-CRWA[,-which(names(CRWA)=="SampleTime")] #delete time field
  #Add AnalysisHour  AnalysisMinutes	
    CRWA$AnalysisTime[CRWA$AnalysisTime==""]<-"NA:NA" #replace missing values
    a<-matrix(as.numeric(unlist(strsplit(CRWA$AnalysisTime,':'))),nrow=nrow(CRWA),ncol=2,byrow=TRUE) #parse
    CRWA$AnalysisHour<-a[,1]
    CRWA$AnalysisMinutes<-a[,2]
    CRWA<-CRWA[,-which(names(CRWA)=="AnalysisTime")] #delete time field
  #Add Units and update parameter
    CRWA$Units<-'ug/l'
  #Add Rep
    CRWA$Rep<-1
  #Change yes/no field to True/False: Frozen Filtered  Photos	Flag
    #Frozen
      table(CRWA$Frozen1,useNA='ifany')
      CRWA$Frozen<-ifelse(CRWA$Frozen1=="Yes",TRUE,FALSE)
      table(CRWA$Frozen,useNA='ifany')
      CRWA<-CRWA[,-which(names(CRWA)=="Frozen1")] #delete old field
    #Filtered
      table(CRWA$Filtered1,useNA='ifany')
      CRWA$Filtered<-ifelse(CRWA$Filtered1=="YES",TRUE,FALSE)
      table(CRWA$Filtered,useNA='ifany')
      CRWA<-CRWA[,-which(names(CRWA)=="Filtered1")] #delete old field
    #Photos
      table(CRWA$Photos1,useNA='ifany')
      CRWA$Photos<-ifelse(CRWA$Photos1=="YES",TRUE,FALSE)
      table(CRWA$Photos,useNA='ifany')
      CRWA<-CRWA[,-which(names(CRWA)=="Photos1")] #delete old field
    #Flag
      table(CRWA$Flag1,useNA='ifany')
      CRWA$Flag<-FALSE
      table(CRWA$Flag,useNA='ifany')
      CRWA<-CRWA[,-which(names(CRWA)=="Flag1")] #delete old field
  #reorder fields to match template
    CRWA<-CRWA[,names(VTDEC)]
# add to data.frame Data2014
  Data2014<-rbind(Data2014,CRWA)
```

### New Hampshire

* Data received from Sonya Carlson on 11/19/14
* Original file name: NH-2014-11-19-PhycoData.xlsx renamed NHDES.xlsx
* Additional Location information received from Sonya as 'NHDES_MissingLocations.csv' on 1/9/15


**Questions for Sonya: with her responses**

* The locations are in tab "List_Frame_1" and the Data are in the data tab.  Both sets of data appear to be linked by the first column ("#" or "Sample #").  It appears that a some of the lines are out of order (i.e., 2 & 3 and 83 & 84).  This is not a problem I just adjusted them.  Is this okay?
    - SC: Okay
* In the data tab there are 2 entries for Sample#=42.  The first for DESStationID=BEELTUFD is not in the List_Frame_1 tab but the second (DESStationID=BROOSSD) is.  To correct this I used the location information from Sample#=54 (DESStationID=BEELTUFD).  Is this okay?
    - SC: Okay
* For Sample#=135 the DES Station ID is "TOWCAN-GEN
" in the Data tab and "TOWCND-GEN" in the List_Frame_1 tab.  Which is correct?
    - SC: TOWCND-GEN: this is corrected above
* Could you verify the units for the Beagle readings.  You have mg/l but others report µg/L.
    - SC: Should be µg/L-corrected above.
* Are all Turner readings in RFU and all beagle in µg/L?  
    - SC: Yes
* Were all samples except the Grab samples collected with  an integrated sampler (a hose)?
    - SC: Yes


**Data steps**-View code in Data2014.rmd document to see details.

* The fluorometer data are in NHDES.XLSX tab:Data. This is copied to NHDES_RAW.csv and modified as follows:
  - First column ("#") containing a sequential list of numbers is renamed "Order"
  - There are two observations for Order=42;the first does not match the location information but the second does.
  - First occurence of Order=42 changed to Order=144; it is then cut and pasted to the bottom of the sheet.
  - The location information for the new NHDES_Raw:Order=144 data matches NHDES_Locations:Order=54. See NHDES_Locations.csv below. 
  - Order=135 the DES Station ID doesn't match the Location table.  I checked with Sonya and changed the NHDES_RAW value to 'TOWCND-GEN'.
  - The "adjusted turner" and calculated "ratio" fields were deleted.
  - The value fields for the Fluorometry readings are on three lines.  These were combined into a single field name using the following convention: Pigment_Units_Fluorometer_SampleState_FilterState.  These will be split up and renamed later.
* Location information is in NHDES.XLSX tab:List_Frame1.  This is copied to NHDES_Locations.csv and modified as follows:
    - First column ("#") containing a sequential list of numbers is renamed "Order"
    - Missing location information for NHDES_Raw:Order=144 same as information NHDES_Locations:Order=54.  This line is copied and appended to the bottom and the Locations:Order is Changed to 144
    - Observations for Order=2 and Order=3 are reversed between NHDES_Raw.csv and NHDES_Data.csv.  Locations:Order=2 changed to 3 and Locations:Order=3 changed to 2.
    - Observations for Order=83 and Order=84 are reversed between NHDES_Raw.csv and NHDES_Data.csv.  Locations:Order=83 changed to 84 and Locations:Order=84 changed to 83.

```{r NHDES, include=FALSE, echo=FALSE, cache=FALSE} 
#read the data
  NH<-read.csv("NHDES_RAW.csv",stringsAsFactors=FALSE) #read the data
  Loc<-read.csv("NHDES_Locations.csv") #read the data
    str(Loc) #Longitude read as character data.  Observation #88 seems to be the problem
    data.frame(a=Loc$Longitude.DecDeg,b=as.numeric(Loc$Longitude.DecDeg))  #-71.4010500
      Loc$Longitude.DecDeg[88]<--71.4010500  #not sure why this is read as a number but it can be fixed
    Loc$Longitude.DecDeg<-as.numeric(Loc$Longitude.DecDeg)
#compare Order in NH and LOC
  all.equal(NH$Order,Loc$Order) #TRUE
#Build NHDES
  #Build Template first
    NH_T<-data.frame(Order=NH$Order)
    NH_T$YourName<-"Sonya Carlson"
    NH_T$Organization<-'NHDES'
    NH_T$SiteID<-NH$DES.Station.ID
    NH_T$WaterbodyName<-Loc$Waterbody  #this is a number but appears to be consistent names is StatonName field
    NH_T$State<-'NH'
    NH_T$SiteLocation<-Loc$Station.Name
    # Add SampleYear  SampleMonth	SampleDay	
        a<-matrix(as.numeric(unlist(strsplit(NH$Sample.Date,'/'))),nrow=nrow(NH),ncol=3,byrow=TRUE)
          NH_T$SampleYear<-a[,3] 
          NH_T$SampleMonth<-a[,1] 
          NH_T$SampleDay<-a[,2] 
    NH_T$NameOfSamplers<-NA
    NH_T$WeatherConditions<-NH$Weather
    #Add SampleLocation
      table(NH$EPA.Designation,useNA='ifany')
      NH_T$SampleLocation<-NH$EPA.Designation
        NH_T$SampleLocation[NH_T$SampleLocation=='SS#1']<-'SS1'
        NH_T$SampleLocation[NH_T$SampleLocation=='SS#2']<-'SS2'
        NH_T$SampleLocation[NH_T$SampleLocation=='SS#3']<-'SS3'
        NH_T$SampleLocation[NH_T$SampleLocation=='WL#1']<-'WL1'
        NH_T$SampleLocation[NH_T$SampleLocation=='WL#2']<-'WL2'
        NH_T$SampleLocation[NH_T$SampleLocation=='WL#3']<-'WL3'
        NH_T$SampleLocation[NH_T$SampleLocation=='WL#4']<-'WL4'
        NH_T$SampleLocation[NH_T$SampleLocation=='GRAB']<-'Other' #change SampleMethod to 'Grab'
      table(NH_T$SampleLocation,useNA='ifany')
    #Add SampleMethod
      NH_T$SampleMethod<-'Integrated'
        NH_T$SampleMethod[NH_T$SampleLocation=='Other']<-'Grab'
    NH_T$Depth<-NH$Depth
    NH_T$Latitude<-Loc$Latitude.DecDeg
    NH_T$Longitude<-Loc$Longitude.DecDeg
    NH_T$SampleHour<-NA 
    NH_T$SampleMinutes<-NA 
    NH_T$Filtered<-NA 
    NH_T$Frozen<-NA 
    NH_T$Parameter<-NA 
    NH_T$Value<-NA 
    NH_T$Units<-NA 
    NH_T$Rep<-NA
    NH_T$Fluorometer<-NA
    NH_T$AnalysisYear<-NA 
    NH_T$AnalysisMonth<-NA
    NH_T$AnalysisDay<-NA
    NH_T$AnalysisHour<-NA
    NH_T$AnalysisMinutes<-NA
    NH_T$GPSType<-NA
    NH_T$Photos<-FALSE
    NH_T$Flag<-FALSE
    NH_T$Comments<-NA
#Loop to add "Filtered"  "Frozen" "Parameter" "Value" "Units" & "Fluorometer"
  NHDES<-c()  #empty data frame
    for(i in c(7:ncol(NH))){
      a<-unlist(strsplit(names(NH)[i],'_')) #get the column name
      b<-NH_T #open the blank template then fill in the field below
        b$Parameter<-a[1]
        b$Units<-a[2]
        b$Fluorometer<-a[3]
        b$Frozen<-ifelse(a[4]=='Frozen','TRUE','FALSE')
        b$Filtered<-ifelse(a[5]=='Filtered','TRUE','FALSE')
        b$Value<-NH[,i]
      NHDES<-rbind(NHDES,b) #add data to data.frame
    }
```

* update missing locations with information received from Sonya Carlson on 1/9/15
  - Locations in "NHDES_MissingLocations.csv"
  - Received locations for 6 locations.
  - No locations fro Rockybound Pond SS#1 and SS#2 near the town of Croydon NH
  - Approximate locations for these sites is the approximate centroid of the lake.
  - Update Lat/Lon 
  - Add comments
* Add to data.frame Data2014

```{r NHloc, include=FALSE, echo=FALSE, cache=FALSE} 
#read data 
  NHmissing<-read.csv("NHDES_MissingLocations.csv") #additional location information.
    NHmissing<-NHmissing[,c("Order","DES.Station.ID","DES.Alias.ID","Station.Name","Town","Station.Type","Latitude.DecDeg","Longitude.DecDeg" )]
#update Lat & Lon
  a<-which(NHDES$Order%in%c(52,53,108,112,113,114,115,123)) #row indices of locations to update
  b<-NHDES[a,c('Order','SiteID','WaterbodyName')]  #rows to update
    b$RowName<-a  #use this to keep in the same order
  b<-merge(b,NHmissing,by='Order',all.x=TRUE) #add missing lat/lon
          b<-b[order(b$RowName),]  #reorder 
        NHDES[a,c('Latitude','Longitude')]<-b[,c('Latitude.DecDeg','Longitude.DecDeg')]   
          #update Comments
  #consistency check
      unique(NHDES[a,c('Order','Latitude')])==NHmissing[,c('Order','Latitude.DecDeg')]

# add to data.frame Data2014
  Data2014<-rbind(Data2014,NHDES)
```

### Connecticut

* Data received from Tracy Lizotte on 11/17/14
* Original file name: "Cyanobacteria data Summer 2014.xlsx" saved as CTDEEP.xlsx
* had trouble reading the .xlsx file so file resaved as CTDEEP.XLS

Questions for Tracy:

* For the Housatonic River (Kettletown) on 7/7/2014 the fluorometer values are "337.23//254.39", "310.87//363.l59", "24.91//7.50", and "2.50//5.50".  Do these represent multiple observations from the same sample or perhaps readings from 2 different samples? For now I am calling these reps and have separated the values.
    - TL 1/7/14: These are multiple observations from the same sample.
* What are the measurement units? RFU, µg/L?
    - TL 1/7/14: The measurements are in RFU.
* Where were the samples taken?  Shoreside? or Within Lake?
    - TL 1/7/14: All samples were taken at shoreside .
* What depth were the samples taken at?
    - TL 1/7/14: Samples procedures called for taking the integrated samples at approximately 1.0 m.
* How were the samples taken?  Grab samples? Whole Water Samples? Integrated Tube Samples?
    - TL 1/7/14: Samples were composite grab samples taken with a Modified Integrated Sample Tube. 
* I have some but not all of the Latitude and Longitudes for the lakes.  Please check the file CTDEEP_Locations.csv and verify the locations (I can send a map of the locations if you wish) and fill in missing values. 
    - TL 1/7/14: Working on it.
* Were any samples frozen?
    - TL 1/7/14:  No, the results that you have at this point are all from as is sample.

**Data steps**-View code in Data2014.rmd document to see details.
* Data organized with a separate lake in each tab.  These are read directly into R and the data from the separate sheets are combined into a single data.frame.
```{r CTDEEP1, include=FALSE, echo=FALSE, cache=FALSE} 
#read the data from the Excel files and rbind all of the separate sheets into a single DF
library(gdata)
  CT<-c() #blank object to store data
    for(i in c(1:11)){
      b<-read.xls ("CTDEEP.xls", sheet = i, header = TRUE) # read a sheet
      CT<-rbind(CT,b)
    }
```
* Each lake has a parameter field that includes many parameters besides the fluorometry data but most of these are missing values.
* The dataset includes many parameters but only the Chlorophyll and Phycocanin observations will be kept.
* The Parameters with Chlorophyll and Phycocyanin include information on whether the samples were filtered or not. We'll assume that the samples were not frozen.
* The parameter field was parsed as follows (NOTE: observations with ParameterNew=NA were deleted):
```{r CTDEEP2, include=TRUE, echo=FALSE, cache=FALSE} 
  #Add ParameterNew field with just the Parameter names
    CT$ParameterNew<-NA
    CT$ParameterNew[grep('Phycocyanin',CT$Parameter)]<-'Phycocyanin' 
    CT$ParameterNew[grep('Chl a',CT$Parameter)]<-'Chlorophyll' 
  #add Frozen and Filtered fields based on Parameter names
    a<-c(grep("As is",CT$Parameter), grep("unfiltered",CT$Parameter)) #select parameter for unfiltered samples
    CT$Frozen<-FALSE  #all samples appear to have been processed unfrozen
    CT$Filtered<-FALSE #start with all equal FALSE
      CT$Filtered[-a]<-TRUE #changed to TRUE those that do not have "unfiltered" or "AS is" in the parameter name.
  #Show changes
    unique(CT[order(CT$Parameter),c('Parameter','ParameterNew','Filtered','Frozen')])
  #Keep phycocyanin and Chlorophyll data only
   CT<-CT[!is.na(CT$ParameterNew),] #limit to Phyco and Chl parameters only
```
* The value field contains a mix of numeric values, character values, and missing values.  
* The table below shows the number of occurences of each non-numeric value
```{r CTDEEP3, include=TRUE, echo=FALSE, cache=FALSE, warning=FALSE} 
  CT$ValueNew<-as.numeric(as.character(CT$Value))
  as.data.frame(table(as.character(CT$Value[is.na(CT$ValueNew)]),useNA='ifany'))
```
* The four character values with two number separated by 2 slashes are two readings for the same sample.  Split these into two lines.  Assign the first to Rep=1 with Value = the first value.  Assign the second to Rep=2 with Value = the second value. 
* Delete observations with Value="not Collected" or " "
```{r CTDEEP4, include=FALSE, echo=FALSE, cache=FALSE} 
  #add Rep field
    CT$Rep<-1
  #add flag field
    CT$Flag<-FALSE
  #add comment field
    CT$Comment<-NA
  #copy lines with two readings;change ValueNew to second reading and the Rep to Rep=2
    a<-CT[grep("//",CT$Value),] #select rows
    a$Rep<-2 #change rep to 2
    a$ValueNew<-c(254.39,363.159,7.50,5.50) #Note: 363.l59 changed to 363.159
    a$Flag<-TRUE  #add flag
    a$Comment<-paste('Value Changed from:',a$Value) #add comment
  #change ValueNew for lines with two readings for value to the first reading
    b<-grep("//",CT$Value)
    CT$Flag[b]<-TRUE
    CT$Comment[b]<-paste('Value Changed from:',CT$Value[b])
    CT$ValueNew[b]<-c(337.23,310.87,24.91,2.50)
    CT[b,]
  #Append rep=2 lines to CT
    CT<-rbind(CT,a)
  #Delete rows with ValueNew=NA
    nrow(CT) #603+4=607
    CT<-CT[!is.na(CT$ValueNew),]
    nrow(CT) #252
```
* Data massage complete-build CTDEEP
* Add to data.frame Data2014
* Missing CTDEEP locations received from Tracy Lizotte on 1/8/14 (CTDEEP_Locations.csv)
* Latitude and Longitude updated
```{r CTDEEP, include=FALSE, echo=FALSE, cache=FALSE} 
#rename fields to remove spaces and special characters.
names(CT)<-c("StationID","WaterbodyName","Landmark","Date","lab_accession","Parameter","Value","gtG","ltG","UOM","Lab","Note","ParameterNew","Frozen","Filtered","ValueNew","Rep","Flag","Comment")
#view data
  unique(CT$StationID) #SiteID: 12 station IDs 
  unique(CT$WaterbodyName) #WaterbodyName 12 lake names
  unique(CT$Landmark) #SiteLocation 
  unique(CT$Date) #SampleDates in format YYYY-MM-DD
  unique(CT$lab_accession) #all NA
  table(CT$gtG,useNA='ifany') #this must be the detection limit
  table(CT$ltG,useNA='ifany') #this must be the detection limit
  unique(CT$UOM) #Units: NA
  unique(CT$Lab) # all values="DEEP"
  unique(CT$Note) #"Beagle Flourometer"
#build CTDEEP
  CTDEEP<-data.frame(Order=1:nrow(CT))
  CTDEEP$YourName<-'Tracy Lizotte'
  CTDEEP$Organization<-'CTDEEP'
  CTDEEP$SiteID<-CT$StationID
  CTDEEP$WaterbodyName<-CT$WaterbodyName
  CTDEEP$State<-'CT'
  CTDEEP$SiteLocation<-CT$Landmark
  CTDEEP$SampleYear<-2014 
  CTDEEP$SampleMonth<-as.numeric(substr(CT$Date,6,7)) 
  CTDEEP$SampleDay<-as.numeric(substr(CT$Date,9,10)) 
  CTDEEP$NameOfSamplers<-NA
  CTDEEP$WeatherConditions<-NA
  CTDEEP$SampleLocation<-'SS1'
  CTDEEP$SampleMethod<-"Integrated"
  CTDEEP$Depth<-1
  CTDEEP$Latitude<-NA
  CTDEEP$Longitude<-NA
  CTDEEP$SampleHour<-NA
  CTDEEP$SampleMinutes<-NA
  CTDEEP$Filtered<-CT$Filtered
  CTDEEP$Frozen<-CT$Frozen
  CTDEEP$Parameter<-CT$ParameterNew
  CTDEEP$Value<-CT$ValueNew
  CTDEEP$Units<-'RFU'
  CTDEEP$Rep<-CT$Rep
  CTDEEP$Fluorometer<-'Beagle'
  CTDEEP$AnalysisYear<-NA
  CTDEEP$AnalysisMonth<-NA
  CTDEEP$AnalysisDay<-NA
  CTDEEP$AnalysisHour<-NA
  CTDEEP$AnalysisMinutes<-NA
  CTDEEP$GPSType<-NA
  CTDEEP$Photos<-FALSE
  CTDEEP$Flag<-CT$Flag
  CTDEEP$Comments<-CT$Comment

#add location information
  CTL<-read.csv("CTDEEP_locations.csv",stringsAsFactors=FALSE) #read the data
    CTL<-CTL[,c('SiteID','Latitude','Longitude')] #keep only the fields of interest
    a<-CTDEEP[,c('Order','SiteID')]  #get merge fields from CTDEEP
    a<-merge(a,CTL,by='SiteID',all.x=T) #merge locations with CTDEEP
    a<-a[order(a$Order),]  #reorder
    all.equal(a$Order,CTDEEP$Order) #check order
    #Update fields
      CTDEEP$Latitude<-a$Latitude
      CTDEEP$Longitude<-a$Longitude

# add to data.frame Data2014
  Data2014<-rbind(Data2014,CTDEEP)
```

### University of New Hampshire

* Data received from Amanda Murby on 12/29/14
* Original file name: "Region1CyanobacteriaDataEntryTemplate_NewHampshire-UNH2014.xls" saved as UNH.xls
* Some updated locations received from Jim Haney 1/9/15 as "UNH_MissingLocations.xlsx"  saved as UNH_MissingLocations.csv - more information on this below
 
**Questions for Amanda:**
I've been modifying the template as I process data.  One of the fields added since you got the template is a column for "SampleMethod" that identifies how the sample was collected (Integrated Tube Sample, Grab Sample, etc.).  Several of the questions below relate to this and others not. The questions are: 

* The "Depth"" field has a mixture of depth ranges and single depth values.  Do these represent different sample methods?  For example are the depth ranges (e.g., 0-4) from the integrated tube samplers?  How were the single depth samples collected? 
    - AM 12/31/15: Those with 0-4 or other are integrated. Those as 0 were surface samples by grab. Those with just one depth were sampled by bottle, such as a Van Dorn, discrete water sampler.
* The "TypeOfSample" field has many missing values. What are these samples? and, How were they collected?
    - AM 12/31/15: No response
* What are the measurement units? RFU? µg/L?
    - AM 12/31/15: RFU
* What do the Flags=Yes represent? Most comments for these lines are blank.
    - AM 12/31/15: see answer below
* Missing Longitude and Latitude?
    - AM 12/31/15: We had to google for approx. GPS. However, I didn't know the exact long and lat for the lakes with multiple sites. I flagged yes for those that did not have lat/long yet. I could find out from those who sampled. I should have copied the same comment for all of those flagged. 
    - BM add comment "Location Approximate or Missing" to Flag=YES observations
* Was the Beagle used for all readings
    -AM 1/14/15 Yes for the data sent. 

**Data steps**-View code in Data2014.rmd document to see details.

* Data originally entered in a version of Region1CyanobacteriaDataEntryTemplate.xls 
* UNH_RAW.csv imported into R
* Each field verified with consistency checks
* Fields updated where necessary.

```{r UNH, include=FALSE, echo=FALSE, cache=FALSE} 
#read data 
  UNH<-read.csv("UNH_RAW.csv",stringsAsFactors=FALSE) #read the data
  UNHmissing<-read.csv("UNH_MissingLocations.csv") #additional location information.
  names(UNH)[c(15,16,23,24)]<-c("Filtered","Frozen","Photos","Flag") #rename fields that ended in "?"
#remove empty line.
  UNH[338,] #this line contains only the comment "additional data missing" delete the line
  UNH<-UNH[-338,]
#check fields X,X.1-X.17
    table(UNH$X,useNA='ifany') # Appears to be the 6 missing negative values for a blank reading-delete
    table(is.na(UNH[,paste("X.",c(1:17),sep='')]))  #all NA; delete
    UNH<-UNH[,-c(26:ncol(UNH))]
#Review, add, & modify fields as needed
  UNH$Order<-1:nrow(UNH)
  table(UNH$YourName,useNA='ifany') 
  table(UNH$Organization,useNA='ifany')  #'UNH Center for Freshwater Biology' change to:
    UNH$Organization<-'UNH_CFB'
  table(UNH$SiteID,useNA='ifany')  #all NA
  table(UNH$WaterbodyName,useNA='ifany')
    UNH$WaterbodyName[UNH$WaterbodyName=='']<-NA #replace empty cells with NA
  table(UNH$State,useNA='ifany')  #ME=12 NH=313 NA=12
    UNH$State[UNH$State=='']<-NA #replace empty cells with NA
  table(UNH$SiteLocation,useNA='ifany')
    UNH$SiteLocation[UNH$SiteLocation=='']<-NA #replace empty cells with NA
  # Add SampleYear  SampleMonth  SampleDay	Drop SampleDate
    table(UNH$SampleDate,useNA='ifany')
      UNH$SampleDate[UNH$SampleDate=='']<-'NA/NA/NA' #replace empty cells with NA/NA/NA
    a<-matrix(as.numeric(unlist(strsplit(UNH$SampleDate,'/'))),nrow=nrow(UNH),ncol=3,byrow=TRUE)
      UNH$SampleYear<-a[,3] 
      UNH$SampleMonth<-a[,1] 
      UNH$SampleDay<-a[,2]
      UNH<-UNH[,-which(names(UNH)=="SampleDate")]
  table(UNH$NameOfSamplers,useNA='ifany')
    UNH$NameOfSamplers[UNH$NameOfSamplers=='']<-NA #replace empty cells with NA
  table(UNH$WeatherConditions,useNA='ifany')
    UNH$WeatherConditions[UNH$WeatherConditions=='']<-NA #replace empty cells with NA
  #rename TypeOfSample to SampleLocation
    names(UNH)[which(names(UNH)=="TypeOfSample")]<-'SampleLocation'
      table(UNH$SampleLocation,useNA='ifany')
        #modify values for consistency
          UNH$SampleLocation[UNH$SampleLocation=='']<-NA #replace empty cells with NA
          unique(Data2014$SampleLocation)
            unique(UNH$SampleLocation)
              UNH$SampleLocation<-gsub("-","",UNH$SampleLocation)
              UNH$SampleLocation<-gsub("Standard","Calibration",UNH$SampleLocation)
  #Sample Method, SampleLocation and Depth are conflated in this version of the template.
    #SampleLocation is Okay as is but:
      #SampleMethod for Depth expressed as integer values are:
        #SampleMethod='VanDorn' for Depth>0
          UNH$SampleMethod[!is.na(as.numeric(UNH$Depth))]<-'VanDorn'
        #SampleMethod='Grab' for Depth=0
          UNH$SampleMethod[which(UNH$Depth=='0')]<-'Grab'
  #change field Flag from Y/N to TRUE/FALSE
      table(UNH$Flag,useNA='ifany')
        #Flag=TRUE observations have missing or approximate locations. These can be eliminated
          #Change all to Flag=TRUE and convert to logical
            UNH$Flag<-FALSE
            UNH$Flag<-as.logical(UNH$Flag) #convert to logical
    #Depths expressed as ranges are for SampleMethod='Integrated'; change depth to the max for the range.
      #change ranges to max
        a<-grep("-",UNH$Depth) #select rows with depth ranges
          as.data.frame(table(UNH$Depth[a],useNA='ifany'))  #check
        b<-as.numeric(unlist(strsplit(UNH$Depth[a],'-'))) #use the "-" character to split values
        #b[seq(2,length(b),2)] #choose the second value only
          data.frame(UNH$Depth[a],b[seq(2,length(b),2)]) #check
        #update Depth to max and SampleMethod='Integrate' and add comment
          UNH$Comments[a]<-paste("Depth Changed From:",UNH$Depth[a])
          UNH$Depth[a]<-b[seq(2,length(b),2)] #update Depth
          UNH$SampleMethod[a]<-'Integrated' #update SampleMethod
          UNH$Flag[a]<-TRUE
    #missing values for depths are for Calibration standards and blanks
      #Replace Depth='' with Depth=NA
        UNH$Depth[UNH$Depth=='']<-NA #replace empty cells with NA
      #Convert UNH$Depth to numeric
        UNH$Depth<-as.numeric(UNH$Depth)
      #Update SampleMethod='Calibration' for SampleLocation='Calibration';
        table(subset(UNH$Depth,UNH$SampleLocation=='Calibration'),useNA='ifany') #6 obs. for Depth=NA
        UNH$SampleMethod[UNH$SampleLocation=='Calibration']<-'Calibration' 
      #Update SampleMethod='Blank' for SampleLocation='Blank'
        table(subset(UNH$Depth,UNH$SampleLocation=='Blank'),useNA='ifany') #61 obs. for Depth=NA & 4 obs. for Depth=1
        UNH$SampleMethod[UNH$SampleLocation=='Blank']<-'Blank' 
      #Verify
        table(UNH$SampleMethod,useNA='ifany')
  #continue with template
    table(UNH$Latitude,useNA='ifany')  #missing 105 observations
    table(UNH$Longitude,useNA='ifany') #missing 105 observations
  #parse SampleTime
      table(UNH$SampleTime,useNA='ifany') #All missing
        UNH$SampleHour<-NA
        UNH$SampleMinutes<-NA
          UNH<-UNH[,-which(names(UNH)=="SampleTime")]
    table(UNH$Filtered,useNA='ifany')
    #change field Filtered from Y/N to TRUE/FALSE
      table(UNH$Filtered,useNA='ifany')
        UNH$Filtered[UNH$Filtered=='']<-NA #replace empty cells with NA
        UNH$Filtered<-gsub("Yes",TRUE,UNH$Filtered)
        UNH$Filtered<-gsub("No",FALSE,UNH$Filtered)
          UNH$Filtered<-as.logical(UNH$Filtered) #convert to logical
    #change field Frozen from Y/N to TRUE/FALSE
      table(UNH$Frozen,useNA='ifany')
        UNH$Frozen[UNH$Frozen=='']<-NA #replace empty cells with NA
        UNH$Frozen<-gsub("Yes",TRUE,UNH$Frozen)
        UNH$Frozen<-gsub("No",FALSE,UNH$Frozen)
          UNH$Frozen<-as.logical(UNH$Frozen) #convert to logical
    table(UNH$Parameter,useNA='ifany')
    table(UNH$Value,useNA='ifany')
    UNH$Units<-'RFU'
    UNH$Rep<-NA
    table(UNH$Fluorometer,useNA='ifany')  #mostly missing but Amanda confirmed they were all done with Beagle
      UNH$Fluorometer<-'Beagle'
    # Add AnalysisYear  AnalysisMonth  AnalysisDay  Drop AnalysisDate
      table(UNH$AnalysisDate,useNA='ifany')
        UNH$AnalysisDate[UNH$AnalysisDate=='']<-'NA/NA/NA' #replace empty cells with NA/NA/NA
      a<-matrix(as.numeric(unlist(strsplit(UNH$AnalysisDate,'/'))),nrow=nrow(UNH),ncol=3,byrow=TRUE)
        UNH$AnalysisYear<-a[,3] 
        UNH$AnalysisMonth<-a[,1] 
        UNH$AnalysisDay<-a[,2]
        UNH<-UNH[,-which(names(UNH)=="AnalysisDate")]
    #parse AnalysisTime
      table(UNH$AnalysisTime,useNA='ifany') #All missing
        UNH$AnalysisHour<-NA
        UNH$AnalysisMinutes<-NA
          UNH<-UNH[,-which(names(UNH)=="AnalysisTime")]
    table(UNH$GPSType,useNA='ifany')
      UNH$GPSType[UNH$GPSType=='']<-NA #replace empty cells with NA
    #change field Photos from Y/N to TRUE/FALSE
      table(UNH$Photos,useNA='ifany')
        UNH$Photos[UNH$Photos=='']<-NA #replace empty cells with NA
        UNH$Photos<-gsub("Yes",TRUE,UNH$Photos)
        UNH$Photos<-gsub("No",FALSE,UNH$Photos)
          UNH$Photos<-as.logical(UNH$Photos) #convert to logical
      
  #reorder fields to match Data2014
    #consistency check
    ncol(UNH) #35
    ncol(Data2014) #35
    table(names(UNH)%in%names(Data2014))
    names(UNH)[27]
    #reorder
      UNH<-UNH[,names(Data2014)]
```

* Jim Haney has informed us (1/12/15) that the site identified as UNH$WaterbodyName=='Wentworth' & UNH$SiteLocation=='Center' in the orginal dataset sent by Amanda should be changed to UNH$WaterbodyName=='Crescent Lake' & UNH$SiteLocation=='Center' 

```{r UNHname, include=FALSE, echo=FALSE, cache=FALSE} 
  a<-which(UNH$WaterbodyName=='Wentworth' & UNH$SiteLocation=='Center') #select observations
  UNH$WaterbodyName[a]<-'Crescent Lake' #change name
  UNH$Comments[a]<-paste(UNH$Comments[a],"; WaterbodyName changed from 'Wentworth'")
``` 

* update missing locations with information received from Jim Haney 1/9/15 (UNH_MissingLocations.csv)
* location for Lake Wentworth SiteLocation=NA still missing; set to approximate lake centroid. (Latitude=43.597897 & Longitude=-71.157598)
* Update Lat/Lon 
* Add to data.frame Data2014

```{r UNHloc, include=FALSE, echo=FALSE, cache=FALSE} 
#read data 
  UNHmissing<-read.csv("UNH_MissingLocations.csv") #additional location information.
#update missing lake location
  UNHmissing[which(is.na(UNHmissing$SiteLocation==TRUE)),c('Latitude','Longitude')]<-c(43.597897,-71.157598)
#update Lat & Lon
        a<-which(is.na(UNH$Latitude) & UNH$SampleMethod=="Integrated") #row indices of locations to update
        b<-UNH[a,c('WaterbodyName','SiteLocation')]  #rows to update
          b$RowName<-a  #use this to keep in the same order
        b<-merge(b,UNHmissing,by=c('WaterbodyName','SiteLocation'),all.x=TRUE) #add missing lat/lon
          b<-b[order(b$RowName),-3]  #reorder and remove "RowName" field
        UNH[a,c('Latitude','Longitude')]<-b[,c('Latitude','Longitude')] 
          #update Comments
            b<-which(UNH$Latitude==43.597897)
            UNH$Comments[b]<-paste(UNH$Comments[b],'; Exact Location unknown, approximate lake centroid used')
        #consistency check
          unique(UNH[a,c('WaterbodyName','SiteLocation','Latitude')])==UNHmissing[,c('WaterbodyName','SiteLocation','Latitude')]
# add to data.frame Data2014
    Data2014<-rbind(Data2014,UNH)
```

### Rhode Island

* Data prepared by Linda Green, Elizabeth Herron, & Vuarnelle Urena
* Data received from Elizabeth Herron on 1/6/15
* Original file name: "Region1CyanobacteriaData_ URI_Watershed_Watch-Sent.xlsx" saved as RIWW.xlsx
* Data also saved as RIWW.csv for import into R

**Questions for RIWW**
* How were the samples collected?  Integrated Sampler?
    - EH 1/7/14: Integrated
* What were the units? 
    - EH 1/7/14: µ/L 

**Data steps**-View code in Data2014.rmd document to see details.

* Data entered in an earlier version of Region1CyanobacteriaDataEntryTemplate.xls 
* RIWW.csv imported into R
* Each field verified with consistency checks
* Field updated where necessary.
* Add to data.frame Data2014

```{r RIWW, include=FALSE, echo=FALSE, cache=FALSE} 
#read data 
  RIWW<-read.csv("RIWW.csv",stringsAsFactors=FALSE) #read the data
  names(RIWW)[c(15,16,23,24)]<-c("Filtered","Frozen","Photos","Flag") #rename fields that ended in "?"
#check fields X,X.1-X.17
    table(RIWW$X,useNA='ifany') # all NA; delete
    table(is.na(RIWW[,paste("X.",c(1:17),sep='')]))  #all NA; delete
    RIWW<-RIWW[,-c(26:ncol(RIWW))]
#Review, add, & modify fields as needed
  RIWW$Order<-1:nrow(RIWW)
  table(RIWW$YourName,useNA='ifany') 
  table(RIWW$Organization,useNA='ifany')  #'URI Watershed Watch' change to:
    RIWW$Organization<-'RIWW'
  table(RIWW$SiteID,useNA='ifany')  #9 sites
  table(RIWW$WaterbodyName,useNA='ifany') #9 names
  table(RIWW$State,useNA='ifany')  #all RI
  table(RIWW$SiteLocation,useNA='ifany') #most are blank
    RIWW$SiteLocation[RIWW$SiteLocation=='']<-NA #replace empty cells with NA
  # Add SampleYear  SampleMonth  SampleDay  Drop SampleDate
    table(RIWW$SampleDate,useNA='ifany')
      a<-matrix(as.numeric(unlist(strsplit(RIWW$SampleDate,'/'))),nrow=nrow(RIWW),ncol=3,byrow=TRUE)
      RIWW$SampleYear<-2014 
      RIWW$SampleMonth<-a[,1] 
      RIWW$SampleDay<-a[,2]
      RIWW<-RIWW[,-which(names(RIWW)=="SampleDate")]
  table(RIWW$NameOfSamplers,useNA='ifany')
  table(RIWW$WeatherConditions,useNA='ifany')
    RIWW$WeatherConditions[RIWW$WeatherConditions=='']<-NA #replace empty cells with NA
  #rename TypeOfSample to SampleLocation
    names(RIWW)[which(names(RIWW)=="TypeOfSample")]<-'SampleLocation'
      table(RIWW$SampleLocation,useNA='ifany')
        #modify values for consistency
          unique(Data2014$SampleLocation)
            unique(RIWW$SampleLocation)
              RIWW$SampleLocation<-gsub("-","",RIWW$SampleLocation)
  table(RIWW$Depth,useNA='ifany')
  RIWW$SampleMethod<-'Integrated'
  table(RIWW$Latitude,useNA='ifany')  #complete
    #Two latitudes are recorded for Pasquisett Pond:40.4274 & 41.4274; From Google Earth 41.4274 is correct.
      RIWW$Latitude[which(RIWW$Latitude==40.4274)]<-41.4274
  table(RIWW$Longitude,useNA='ifany') #complete
  #parse SampleTime
    table(RIWW$SampleTime,useNA='ifany') #174 missing
    RIWW$SampleTime[RIWW$SampleTime==""]<-"NA:NA" #replace missing values
    a<-matrix(as.numeric(unlist(strsplit(RIWW$SampleTime,':'))),nrow=nrow(RIWW),ncol=2,byrow=TRUE) #parse
    RIWW$SampleHour<-a[,1]
    RIWW$SampleMinutes<-a[,2]
    RIWW[,c('SampleTime','SampleHour','SampleMinutes')]
    RIWW<-RIWW[,-which(names(RIWW)=="SampleTime")] #delete time field
  table(RIWW$Filtered,useNA='ifany') #complete
    #change field Filtered from Y/N to TRUE/FALSE
      RIWW$Filtered<-gsub("Yes",TRUE,RIWW$Filtered)
      RIWW$Filtered<-gsub("No",FALSE,RIWW$Filtered)
        RIWW$Filtered<-as.logical(RIWW$Filtered) #convert to logical
  table(RIWW$Frozen,useNA='ifany') #1 missing value
    #change field Frozen from Y/N to TRUE/FALSE
      RIWW$Frozen[RIWW$Frozen=='']<-NA #replace empty cells with NA
      RIWW$Frozen<-gsub("Yes",TRUE,RIWW$Frozen)
      RIWW$Frozen<-gsub("No",FALSE,RIWW$Frozen)
      RIWW$Frozen<-as.logical(RIWW$Frozen) #convert to logical
  table(RIWW$Parameter,useNA='ifany') #ok
  table(RIWW$Value,useNA='ifany') #ok
  RIWW$Units<-'ug/l'
  RIWW$Rep<-NA
  table(RIWW$Fluorometer,useNA='ifany')  #Beagle
  # Add AnalysisYear  AnalysisMonth  AnalysisDay  Drop AnalysisDate
      table(RIWW$AnalysisDate,useNA='ifany')
        RIWW$AnalysisDate[RIWW$AnalysisDate=='']<-'NA/NA/NA' #replace empty cells with NA/NA/NA
      a<-matrix(as.numeric(unlist(strsplit(RIWW$AnalysisDate,'-'))),nrow=nrow(RIWW),ncol=2,byrow=TRUE)
          #note this does okay for the day of the month but not the character value of month
        RIWW$AnalysisYear<-2014 
        RIWW$AnalysisDay<-a[,1]
        RIWW$AnalysisMonth<-NA
          RIWW$AnalysisMonth[grep('Jul',RIWW$AnalysisDate)]<-7
          RIWW$AnalysisMonth[grep('Aug',RIWW$AnalysisDate)]<-8
          RIWW$AnalysisMonth[grep('Sep',RIWW$AnalysisDate)]<-9
          RIWW$AnalysisMonth[grep('Nov',RIWW$AnalysisDate)]<-11
        unique(RIWW[order(RIWW$AnalysisDate),c('AnalysisDate','AnalysisDay','AnalysisMonth')])
      RIWW<-RIWW[,-which(names(RIWW)=="AnalysisDate")]
    #parse AnalysisTime
      table(RIWW$AnalysisTime,useNA='ifany') #All missing
        RIWW$AnalysisHour<-NA
        RIWW$AnalysisMinutes<-NA
          RIWW<-RIWW[,-which(names(RIWW)=="AnalysisTime")]
    table(RIWW$GPSType,useNA='ifany')
    #change field Photos from Y/N to TRUE/FALSE
      table(RIWW$Photos,useNA='ifany')
        RIWW$Photos<-gsub("Yes",TRUE,RIWW$Photos)
        RIWW$Photos<-gsub("No",FALSE,RIWW$Photos)
          RIWW$Photos<-as.logical(RIWW$Photos) #convert to logical
    #change field Flag from Y/N to TRUE/FALSE
      table(RIWW$Flag,useNA='ifany')
        RIWW$Flag[RIWW$Flag!='Yes']<-FALSE
        RIWW$Flag<-gsub("Yes",TRUE,RIWW$Flag)
        RIWW$Flag<-as.logical(RIWW$Flag) #convert to logical
  #reorder fields to match Data2014
    #consistency check
    ncol(RIWW) #35
    ncol(Data2014) #35
    table(names(RIWW)%in%names(Data2014))
    #reorder
      RIWW<-RIWW[,names(Data2014)]
  # add to data.frame Data2014
    Data2014<-rbind(Data2014,RIWW)
```


### Maine

* Data received from Linda Bacon on 3/13/15 as "Cyanotoxin.zip"
* Data extracted and access database "Cyanotoxin.accdb" renamed "MEDEP.accdb"
* Newer version of ME database receieved from Linda Bacon on 3/19/15 also as "Cyanotoxin.zip"
* The 3/13 version renamed MEDEP20150313.accdb
* The 3/19 version renamed MEDEP.accdb


**Questions for Linda Bacon**
* From the "Events" table can you check the MIDAS numbers for eventID=9 and eventID=49?  These are listed as (eventID=9 & lake=Sabattus & midas=2796) and (eventID=49 & lake=Threemile & midas=5418).  Based on the names and closest cities I believe these should be (eventID=9 & lake=Sabattus & midas=3796) and (eventID=49 & lake=Threemile & midas=5416). They look typos to me. 
  - LB: You are correct; they are either typos or were written on the data sheets wrong.
  - BM: this was fixed in the 3/19 version
* In the "Sample" table what do the following "samplecodes" mean: DH1, DH3, SC1, SC2, & SC3? 
  - LB: DH1 and DH3 refer to Deep Hole Station 1 and Deep Hole Station 3; we paired the regional sampling with our regular sampling regime (taking core sample to the bottom of the epilimnion rather than  just to 3 meters).   We also collected three samples of Surface Scums (SC1, SC2, SC3), if/when they were found to get at concentrations a dog might encounter drinking from scums at the shoreline. 
* In the "Sample" table what do type="C" and type="G" mean?  I"m guessing C is the integrated sampler and G is a grab sample. 
  - LB: Yes, exactly.
* In the "Sample" table what do the values of 0-6 mean in the "Flour" column?
  - LB: That Column is “FluorFrozenDays.” Those should ALL be zero…don’t know how the 1-6 crept in (4 different people entered the data).   We didn’t have time to process any frozen samples…
  - BM: this was fixed in the 3/19 version; all zeros now
* In the "Sample" table what are the units for the values in the "result" column? ug/l? or RFU?
  - LB: Very Good Question!   Last summer I thought they were RFUs but in January was told they were not.  I tried to get a straight answer from the group in January and was told that the units were ug/L.  I was told to ‘trust us’ – which I still do not.  What says you???
  - BM:  the distribution of values is consistent with ug/l.  I have asked Linda to check if they have any chlorophyll records for the lakes to verify.
* In the "Sample" table do you have the lat/lon values for sampleID=329 & sampleID=330?  These correspond to eventid=42.
 - LB: Yes.  They were not on the Cyano Field Sheeet but were on the regular field sheet filled out at the deep hole: 44.3674    -69.607899   (note that all DH1 & WL1 samples were taken at the same location; likewise for DH3 and WL1).
 - BM: this was fixed in the 3/19 version
* In the FluoroResults table what do the codes  "FC", "FP", "RC", & "RP" in the "Anal" column mean?
  - LB: FC=Filtered Chlorophyll; FP= Filtered Phycocyanin; RC=Raw Chlorophyll; FC=Filtered Chlorophyll
* Probably related to the question above-where samples frozen or filtered?  How can I tell? 
  - LB: None were Frozen (see the answer to the “Fluor’ question) and the answer above.  
* Also am guessing that codes "FC" & RC" are for Chlorophyll and "FP" & "RP" are for Phycocyanin.  Am I right?
  - LB: Yup; see above.
* How were the sample site locations determined (GPS? GoogleEarth? Map?)
  - LB: Hand-held GPS  units using WGS84 projection with accuracy +/- ~9feet.  

**Data steps**-View code in Data2014.rmd document to see details.

* SQL query used to read all data into R
* Data reformatted



```{r MEDEP, include=FALSE, echo=FALSE, cache=FALSE} 
# Read data
  library(RODBC)  
  con <- odbcConnectAccess2007('MEDEP.accdb')
    ME<- sqlQuery(con, "
    SELECT Event.EventID, Sample.SampleID, FluoroResults.FluoroResultID, Event.lake, Event.midas, Event.station, Event.sampdate, Event.Town, Event.crew, Event.weather, Sample.samplecode, Sample.depth, Sample.type, Sample.lat, Sample.long, Sample.Time, Sample.Photos, Sample.PhotoNum, Sample.FluorFrozenDays, Sample.DateFluor, Sample.TimeFluor, FluoroResults.Anal, FluoroResults.result
FROM (Event LEFT JOIN Sample ON Event.EventID = Sample.eventid) LEFT JOIN FluoroResults ON Sample.SampleID = FluoroResults.SampleID;
    ",stringsAsFactors=FALSE)
    close(con)
    str(ME)

#Review, add, & modify fields as needed
  MEDEP<-data.frame(Order=ME$FluoroResultID)
  MEDEP$YourName<-'LindaBacon'
  MEDEP$Organization<-'MEDEP'
  MEDEP$SiteID<-ME$midas
  MEDEP$WaterbodyName<-ME$lake
    table(MEDEP$WaterbodyName,useNA='ifany')  
  MEDEP$State<-'ME'
  MEDEP$SiteLocation<-ME$station
  MEDEP$SampleYear<-2014
  MEDEP$SampleMonth<-as.numeric(substr(as.Date(ME$sampdate),6,7))
  MEDEP$SampleDay<-as.numeric(substr(as.Date(ME$sampdate),9,10))
  MEDEP$NameOfSamplers<-ME$crew
  MEDEP$WeatherConditions<-ME$weather
  MEDEP$SampleLocation<-ME$samplecode
  MEDEP$Depth<-ME$depth
  table(ME$type,useNA='ifany')  
    MEDEP$SampleMethod<-ifelse(ME$type=='C','Integrated','Grab')
      table(MEDEP$SampleMethod,useNA='ifany') 
  MEDEP$Latitude<-ME$lat
  MEDEP$Longitude<-ME$lon
  MEDEP$SampleHour<-floor(ME$Time/100)
  MEDEP$SampleMinutes<-ME$Time-(100*MEDEP$SampleHour)
  table(ME$Anal,useNA='ifany')
    MEDEP$Filtered<-NA                                          
      MEDEP$Filtered[ME$Anal=='FC'|ME$Anal=='FP']<-TRUE
      MEDEP$Filtered[ME$Anal=='RC'|ME$Anal=='RP']<-FALSE
        table(MEDEP$Filtered,useNA='ifany')
  MEDEP$Frozen<-FALSE                                             
  MEDEP$Parameter<-NA 
    MEDEP$Parameter[ME$Anal=='FC'|ME$Anal=='RC']<-'Chlorophyll'
    MEDEP$Parameter[ME$Anal=='FP'|ME$Anal=='RP']<-'Phycocyanin'
      table(MEDEP$Parameter,useNA='ifany')    
  MEDEP$Value<-ME$result
  MEDEP$Units<-'ug/l'                                             #############OJO
    summary(Data2014$Value[Data2014$Parameter=='Phycocyanin' & Data2014$Units=='ug/l' & Data2014$Organization!='MEDEP'])
    summary(ME$result[ME$Anal=='FP' |ME$Anal=='RP'])
    summary(Data2014$Value[Data2014$Parameter=='Phycocyanin' & Data2014$Units=='RFU' & Data2014$Organization!='MEDEP'])
  MEDEP$Rep<-NA
  MEDEP$Fluorometer<-"Beagle"
  MEDEP$AnalysisYear<-2014 
  MEDEP$AnalysisMonth<-as.numeric(substr(as.Date(ME$DateFluor),6,7))
  MEDEP$AnalysisDay<-as.numeric(substr(as.Date(ME$DateFluor),9,10))
  MEDEP$AnalysisHour<-floor(ME$TimeFluor/100)
  MEDEP$AnalysisMinutes<-ME$TimeFluor-(100*MEDEP$AnalysisHour)
  MEDEP$GPSType<-'Handheld'                                          
    #change field Photos from 0/1 to TRUE/FALSE
      table(ME$Photos,useNA='ifany')
      MEDEP$Photos<-TRUE
      MEDEP$Photos[ME$Photos==0]<-FALSE
      table(MEDEP$Photos,useNA='ifany')
  MEDEP$Flag<-FALSE
  MEDEP$Comments<-NA
  #reorder fields to match Data2014
    #consistency check
    ncol(MEDEP) #35
    ncol(Data2014) #35
    table(names(MEDEP)%in%names(Data2014))
    #reorder
      MEDEP<-MEDEP[,names(Data2014)]
  # add to data.frame Data2014
    Data2014<-rbind(Data2014,MEDEP)
```

#add observation and location IDs

```{r IDs, include=FALSE, echo=FALSE, cache=FALSE,eval=TRUE} 
# Add ID
  Data2014$ID<-1:nrow(Data2014)
# Add LocID
  a<-na.exclude(unique(Data2014[,c('Longitude','Latitude')])) #633
  a$LocID<-1:nrow(a)
  Data2014<-merge(Data2014,a,by=c('Longitude','Latitude'),all.x=TRUE)
```
 
* Review and standardize the data
* Save as "Data2014.csv" 
  
```{r save, include=FALSE, echo=FALSE, cache=FALSE,eval=TRUE}  

#review the data:
  summary(Data2014$Order)  
  table(Data2014$YourName,useNA='ifany')
  table(Data2014$Organization,useNA='ifany')
  as.data.frame(table(Data2014$SiteID,useNA='ifany'))
  as.data.frame(table(Data2014$WaterbodyName,useNA='ifany'))
  table(Data2014$State,useNA='ifany')  #missing values are 12 UNH calibration readings
  as.data.frame(table(Data2014$SiteLocation,useNA='ifany'))
    Data2014$SiteLocation[Data2014$SiteLocation=='']<-NA #replace empty cells with NA
  Data2014$SampleYear<-2014  #mix of 2014, 14, and NA-make them all 2014
  table(Data2014$SampleMonth,useNA='ifany')
  table(Data2014$SampleDay,useNA='ifany')
  table(Data2014$NameOfSamplers,useNA='ifany')
    Data2014$NameOfSamplers[Data2014$NameOfSamplers=='']<-NA #replace empty cells with NA
  table(Data2014$WeatherConditions,useNA='ifany')
    Data2014$WeatherConditions[Data2014$WeatherConditions=='']<-NA #replace empty cells with NA
  table(Data2014$SampleLocation,useNA='ifany')
  table(Data2014$SampleMethod,useNA='ifany')
  table(Data2014$Depth,useNA='ifany')
    Data2014$Depth[Data2014$Depth=='']<-NA #replace empty cells with NA
  table(Data2014$Latitude,useNA='ifany')
  table(Data2014$Longitude,useNA='ifany')
    Data2014$Longitude[Data2014$Longitude=='']<-NA #replace empty cells with NA
      #a few values for longitude are greater than zero-change to negative values
        a<-!is.na(Data2014$Longitude) & Data2014$Longitude>0
        Data2014$Longitude[a]
        Data2014$Longitude[a]<--Data2014$Longitude[a]     
  table(Data2014$SampleHour,useNA='ifany')  #one ME sample event has erroneus time value (9573) ignore for now
  table(Data2014$SampleMinutes,useNA='ifany') #one ME sample event has erroneus time value (9573) ignore for now
  table(Data2014$Filtered,useNA='ifany')
  table(Data2014$Frozen,useNA='ifany')
  table(Data2014$Parameter,useNA='ifany')
  table(Data2014$Value,useNA='ifany')
  table(Data2014$Rep,useNA='ifany')
  table(Data2014$Fluorometer,useNA='ifany')
  Data2014$AnalysisYear<-2014  #mix of 2014, 14, and NA-make them all 2014
  table(Data2014$AnalysisMonth,useNA='ifany')
  table(Data2014$AnalysisDay,useNA='ifany')
  table(Data2014$AnalysisHour,useNA='ifany')
  table(Data2014$AnalysisMinutes,useNA='ifany')
  table(Data2014$GPSType,useNA='ifany')
  table(Data2014$Photos,useNA='ifany')
  table(Data2014$Flag,useNA='ifany')
  as.data.frame(table(Data2014$Comments,useNA='ifany'))

  table(Data2014$Units,useNA='ifany')
    Data2014$Units<-ifelse(Data2014$Units=='RFU','RFU','ug/l')
  table(Data2014$Units,useNA='ifany')
#save the data
  write.table(Data2014, file='Data2014.csv',row.names=F,sep=',') #write to csv
```







